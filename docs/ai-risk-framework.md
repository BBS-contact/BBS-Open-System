# AI Risk and Safety Framework  
Fundacja BBS — Better Balance System

## 1. Purpose

This document outlines the risk management and safety framework applied to AI-related research and analytical systems developed or evaluated within Fundacja BBS — Better Balance System, including the LEO analytical system.

The framework complements the official AI Governance Policy published in the legal section and provides additional detail for research, grant, and institutional review contexts.

---

## 2. Scope

This framework applies to:

- research and prototype AI systems,
- analytical advisory tools,
- data-driven modeling environments,
- internal testing and laboratory simulations.

AI systems operated by the Foundation function in an advisory capacity and do not exercise autonomous legal or administrative authority.

---

## 3. Risk-Based Approach

AI systems are assessed using a proportional, risk-based methodology aligned with:

- the EU Artificial Intelligence Act framework,
- the European Commission’s Trustworthy AI guidance,
- OECD AI Principles,
- relevant GDPR provisions.

Risk evaluation includes:

- functional risk (impact of incorrect output),
- data protection risk,
- bias and discrimination risk,
- misuse or misinterpretation risk,
- systemic dependency risk.

Higher-risk systems are subject to enhanced documentation, validation, and oversight requirements.

---

## 4. Safety Layers

The Foundation applies layered safeguards proportionate to system risk level, including:

- documented model purpose and scope,
- data source traceability where applicable,
- version control and audit logging,
- structured validation and testing procedures,
- human review for sensitive outputs.

Safety mechanisms are procedural and organizational rather than declarative guarantees.

---

## 5. Human Oversight

AI systems within the Foundation:

- support analytical processes,
- provide structured insights,
- assist in modeling and evaluation tasks.

Final decisions remain under human responsibility.

No fully autonomous decision-making authority is delegated to AI systems in operational contexts.

---

## 6. Transparency and Documentation

Where legally appropriate and operationally safe, the Foundation maintains documentation regarding:

- system architecture (at a general level),
- versioning logic,
- risk mitigation procedures,
- governance responsibilities.

Public disclosures are balanced against intellectual property protection and security considerations.

---

## 7. Data Protection Integration

AI safety mechanisms operate in conjunction with:

- the Privacy Policy,
- the Data Retention Policy,
- internal GDPR compliance procedures.

Where required, Data Protection Impact Assessments (DPIA) are conducted prior to deployment of higher-risk processing systems.

---

## 8. Continuous Review

AI risk management is treated as an ongoing process.

The Foundation periodically reviews:

- regulatory developments,
- technical limitations,
- operational feedback,
- emerging AI safety standards.

Framework updates are implemented where necessary to maintain regulatory alignment and institutional integrity.

---

## 9. Regulatory Context

This framework reflects alignment efforts with:

- Regulation (EU) 2016/679 (GDPR),
- the EU AI Act,
- Polish national legislation,
- applicable research and public-sector innovation standards.

The legally binding framework remains EU and Polish law.

---

**Status:** Research and Governance Framework  
**Version:** 2.0  
**Year:** 2026
